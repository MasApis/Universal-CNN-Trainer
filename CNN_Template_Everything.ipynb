{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzYOIi0aKQUk"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# üöÄ ULTIMATE CNN TEMPLATE: COLAB TURBO EDITION (EFFICIENTNET-B0)\n",
        "# Optimized for: Google Colab GPU (Tesla T4)\n",
        "# Features: Mixed Precision (Speedup), High Batch Size, Auto-Fix Dataset\n",
        "# ==============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import json\n",
        "from google.colab import drive, files\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 0. AKTIFKAN TURBO MODE (MIXED PRECISION) ‚ö°\n",
        "# ==========================================\n",
        "# Ini bikin GPU T4 kerja 2x lebih cepat & hemat memori\n",
        "try:\n",
        "    policy = mixed_precision.Policy('mixed_float16')\n",
        "    mixed_precision.set_global_policy(policy)\n",
        "    print(f\"‚ö° TURBO MODE AKTIF: {policy.compute_dtype}\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Gagal aktifkan Mixed Precision (Gak masalah, lanjut standar).\")"
      ],
      "metadata": {
        "id": "xpztrjzIKqnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. KONFIGURASI \"MAX PERFORMANCE\" üõ†Ô∏è\n",
        "# ==========================================\n",
        "PATH_ZIP_DRIVE = '/content/drive/MyDrive/dataset_final/dataset.zip'\n",
        "MODEL_NAME = 'model_klasifikasi_pro.h5'\n",
        "JSON_NAME = 'labels.json'\n",
        "\n",
        "# Settingan \"Siksa Hardware\"\n",
        "IMG_SIZE = 260   # Resolusi Tinggi (Biar detail sisik ikan kelihatan)\n",
        "BATCH_SIZE = 64  # Kita hajar 64 (GPU T4 Kuat kok!)\n",
        "EPOCHS_1 = 15    # Pemanasan\n",
        "EPOCHS_2 = 40    # Fine Tuning (Lama dikit biar loss 0.1)"
      ],
      "metadata": {
        "id": "IsIEzYuuKvlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. SETUP & AUTO-FIX DATASET üì¶\n",
        "# ==========================================\n",
        "print(\"\\nüöÄ MEMULAI PERSIAPAN DATASET...\")\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "base_dir = '/content/dataset_final'\n",
        "if os.path.exists(base_dir): shutil.rmtree(base_dir)\n",
        "os.makedirs(base_dir)\n",
        "\n",
        "temp_dir = '/content/temp_extract'\n",
        "if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
        "\n",
        "print(f\"üìÇ Mengekstrak & Merapikan: {PATH_ZIP_DRIVE}\")\n",
        "try:\n",
        "    with zipfile.ZipFile(PATH_ZIP_DRIVE, 'r') as zip_ref:\n",
        "        zip_ref.extractall(temp_dir)\n",
        "\n",
        "    # LOGIKA DETEKTIF PENCARI FOLDER KELAS\n",
        "    real_root = None\n",
        "    max_subdirs = 0\n",
        "    for root, dirs, files in os.walk(temp_dir):\n",
        "        valid_dirs = [d for d in dirs if not d.startswith('.')] # Abaikan folder hidden\n",
        "        if len(valid_dirs) > 1:\n",
        "            if len(valid_dirs) > max_subdirs:\n",
        "                max_subdirs = len(valid_dirs)\n",
        "                real_root = root\n",
        "\n",
        "    if real_root:\n",
        "        print(f\"‚úÖ Dataset Ditemukan di: {real_root} ({max_subdirs} Kelas)\")\n",
        "        for item in os.listdir(real_root):\n",
        "            s = os.path.join(real_root, item)\n",
        "            d = os.path.join(base_dir, item)\n",
        "            if os.path.isdir(s): shutil.move(s, d)\n",
        "        shutil.rmtree(temp_dir)\n",
        "    else:\n",
        "        raise Exception(\"Gagal nemu folder kelas. Cek isi ZIP lu bang.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "PfdL6mteK1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATA GENERATOR (MODE ROBOFLOW / PRE-SPLIT) üîÑ\n",
        "#\n",
        "# PAKAI KODE INI JIKA DATASET KALIAN SUDAH TERSEDIA FOLDER Train, Test, Val.\n",
        "# JIKA TIDAK ADA JANGAN PAKAI KODE INI PAKAI KODE YANG DIBAWAH!!!!!!!!!!!\n",
        "# ==========================================\n",
        "print(\"\\nüîÑ Menyiapkan Pipeline Data (Format Roboflow)...\")\n",
        "\n",
        "# 1. Deteksi Lokasi Folder Train & Valid\n",
        "# Roboflow biasanya strukturnya: /dataset/train dan /dataset/valid (atau val)\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "\n",
        "# Cek nama folder validasi (kadang 'valid', kadang 'val')\n",
        "val_dir = os.path.join(base_dir, 'valid')\n",
        "if not os.path.exists(val_dir):\n",
        "    val_dir = os.path.join(base_dir, 'val')\n",
        "\n",
        "# Cek apakah folder beneran ada\n",
        "if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n",
        "    print(\"‚ùå ERROR: Gak nemu folder 'train' atau 'valid'/'val'!\")\n",
        "    print(f\"   Isi folder saat ini: {os.listdir(base_dir)}\")\n",
        "    # Fallback darurat: kalau ternyata strukturnya berantakan, script ini akan stop\n",
        "    raise Exception(\"Struktur folder tidak sesuai format Roboflow Train/Val\")\n",
        "\n",
        "print(f\"üìç Folder Latihan: {train_dir}\")\n",
        "print(f\"üìç Folder Ujian:   {val_dir}\")\n",
        "\n",
        "# 2. Siapkan Generator (TANPA VALIDATION SPLIT)\n",
        "# Kita hapus 'validation_split' karena data sudah terpisah fisik\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "# 3. Load Data\n",
        "print(\"\\nüì• Loading Data Latihan...\")\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,            # <--- Langsung tembak folder TRAIN\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(\"\\nüì• Loading Data Validasi...\")\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,              # <--- Langsung tembak folder VALID\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "rsgKmJc38Cln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATA GENERATOR (FULL SPEED) üîÑ\n",
        "# ==========================================\n",
        "print(\"\\nüîÑ Menyiapkan Pipeline Data...\")\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Gunakan workers=4 buat manfaatin semua Core CPU Colab\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "1JZg9Yg1K-Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. CLASS WEIGHTS (KEADILAN) ‚öñÔ∏è\n",
        "# ==========================================\n",
        "print(\"\\n‚öñÔ∏è Menghitung Bobot Penyeimbang...\")\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_data.classes),\n",
        "    y=train_data.classes\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "9KXi3WYmLHfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. MEMBANGUN MODEL (ARSITEKTUR PRO) üß†\n",
        "# ==========================================\n",
        "print(\"\\nüèóÔ∏è Membangun Arsitektur EfficientNetB0...\")\n",
        "\n",
        "base_model = EfficientNetB0(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Head Model yang Agresif\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x) # Dropout agak gede biar gak cepet overfit pas ngebut\n",
        "x = Dense(512, activation='relu')(x) # Layer tebal biar pinter\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "# PENTING: dtype='float32' wajib di output layer saat pakai Mixed Precision\n",
        "outputs = Dense(train_data.num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "]"
      ],
      "metadata": {
        "id": "SDPRAubQLL_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. TRAINING TAHAP 1 (WARM UP) ü•ä\n",
        "# ==========================================\n",
        "print(f\"\\nüî• MULAI TRAINING TAHAP 1 (Frozen - {EPOCHS_1} Epochs)...\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001, clipnorm=1.0),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history1 = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=EPOCHS_1,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict,\n",
        "    workers=4, # Pakai 4 core CPU buat loading data\n",
        "    use_multiprocessing=True\n",
        ")"
      ],
      "metadata": {
        "id": "_OVj4zAgLTre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. TRAINING TAHAP 2 (FINE TUNING) üöÄ\n",
        "# ==========================================\n",
        "print(f\"\\n‚ùÑÔ∏è MULAI TRAINING TAHAP 2 (Unfreeze - {EPOCHS_2} Epochs)...\")\n",
        "base_model.trainable = True\n",
        "\n",
        "# Label Smoothing buat ngejar loss rendah (0.1 - 0.2)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5, clipnorm=1.0),\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history2 = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=EPOCHS_2,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict,\n",
        "    workers=4,\n",
        "    use_multiprocessing=True\n",
        ")"
      ],
      "metadata": {
        "id": "7ykjDxRbLaRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. SIMPAN & DOWNLOAD OTOMATIS üíæ\n",
        "# ==========================================\n",
        "print(\"\\nüíæ Menyimpan Hasil Kerja Keras...\")\n",
        "\n",
        "# Save Model\n",
        "model.save(MODEL_NAME)\n",
        "\n",
        "# Save JSON\n",
        "kamus_label = {str(v): k.replace(\"_\", \" \").upper() for k, v in train_data.class_indices.items()}\n",
        "with open(JSON_NAME, 'w') as f:\n",
        "    json.dump(kamus_label, f, indent=4)\n",
        "\n",
        "# Plot Grafik\n",
        "acc = history1.history['accuracy'] + history2.history['accuracy']\n",
        "val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
        "loss = history1.history['loss'] + history2.history['loss']\n",
        "val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.title('Akurasi Dewa')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title('Loss (Target 0.1)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('grafik_keren.png') # Simpan grafik juga\n",
        "plt.show()\n",
        "\n",
        "print(\"üéâ MISSION ACCOMPLISHED! Download file dimulai...\")\n",
        "files.download(MODEL_NAME)\n",
        "files.download(JSON_NAME)\n",
        "files.download('grafik_keren.png')"
      ],
      "metadata": {
        "id": "adJBrsVsLekX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}